{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#      MODEL BUILDING PROCESSING\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(r'./data/coords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns = [\"class\"])\n",
    "y = dataset[\"class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "\n",
    "X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\n",
    "# reassingn index and columns\n",
    "X_train2.columns = X_train.columns.values\n",
    "X_train2.index = X_train.index.values\n",
    "X_train_scaled = X_train2\n",
    "\n",
    "X_test2 = pd.DataFrame(sc_X.transform(X_test))\n",
    "X_test2.columns = X_test.columns.values\n",
    "X_test2.index = X_test.index.values\n",
    "X_test_scaled = X_test2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to check perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def perfomance_check(name: str):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, pos_label='positive',\n",
    "                                           average='micro')\n",
    "    rec = recall_score(y_test, y_pred, pos_label='positive',\n",
    "                                           average='micro')\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='positive',\n",
    "                                           average='micro')\n",
    "\n",
    "    model_results = pd.DataFrame([[name, acc, prec, rec, f1]],\n",
    "                   columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "    \n",
    "    return results.append(model_results, ignore_index = True)\n",
    "\n",
    "results = pd.DataFrame(columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# CLASSIFICATIONS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_classifier = LogisticRegression(random_state = 0, penalty = 'l1', solver='saga')\n",
    "LR_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = LR_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('Linear Regression (Lasso)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (K-Nearest Neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "KNN_classifier.fit(X_train_scaled, y_train.values.ravel())\n",
    "\n",
    "y_pred = KNN_classifier.predict(X_test_scaled)\n",
    "\n",
    "results = perfomance_check('K-Nearest Neighbours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVML_classifier = SVC(random_state = 0, kernel = 'linear')\n",
    "SVML_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = SVML_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('SVM (Linear)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernal SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "K_SVM_classifier = SVC(random_state = 0, kernel = 'rbf')\n",
    "K_SVM_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = K_SVM_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('SVM (RBF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB_classifier = GaussianNB()\n",
    "NB_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = NB_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('Naive Bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DTC_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DTC_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = DTC_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('Decision Tree Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_classifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n",
    "                                    criterion = 'entropy')\n",
    "RF_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = RF_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('Random Forest (n=100)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[14:59:54] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1295: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('XGBoost ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from catboost import CatBoostClassifier\n",
    "CB_classifier = CatBoostClassifier()\n",
    "CB_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = CB_classifier.predict(X_test)\n",
    "\n",
    "results = perfomance_check('CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression (Lasso)</td>\n",
       "      <td>0.961240</td>\n",
       "      <td>0.961240</td>\n",
       "      <td>0.961240</td>\n",
       "      <td>0.961240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-Nearest Neighbours</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM (Linear)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM (RBF)</td>\n",
       "      <td>0.922481</td>\n",
       "      <td>0.922481</td>\n",
       "      <td>0.922481</td>\n",
       "      <td>0.922481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree Classification</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (n=100)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K-Nearest Neighbours</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy  Precision    Recall  F1 Score\n",
       "0     Linear Regression (Lasso)  0.961240   0.961240  0.961240  0.961240\n",
       "1          K-Nearest Neighbours  1.000000   1.000000  1.000000  1.000000\n",
       "2                  SVM (Linear)  1.000000   1.000000  1.000000  1.000000\n",
       "3                     SVM (RBF)  0.922481   0.922481  0.922481  0.922481\n",
       "4                   Naive Bayes  1.000000   1.000000  1.000000  1.000000\n",
       "5  Decision Tree Classification  1.000000   1.000000  1.000000  1.000000\n",
       "6         Random Forest (n=100)  1.000000   1.000000  1.000000  1.000000\n",
       "7                      XGBoost   1.000000   1.000000  1.000000  1.000000\n",
       "8          K-Nearest Neighbours  1.000000   1.000000  1.000000  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./saved_model/body_language.pkl', 'wb') as f:\n",
    "    pickle.dump(RF_classifier, f)\n",
    "\n",
    "with open('./saved_model/body_language.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# MODEL SELECTION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross Validation\n",
    "---\n",
    "* Conside which model perform the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=10.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:08] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:10] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:11] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:13] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:14] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:16] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:17] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:19] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:21] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "C:\\Users\\sijal\\anaconda3\\envs\\vision\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "[15:03:22] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Model Accuracy 0: 0.932 (+/- 0.064)\n",
      "Model Accuracy 1: 0.982 (+/- 0.041)\n",
      "Model Accuracy 2: 1.000 (+/- 0.000)\n",
      "Model Accuracy 3: 0.854 (+/- 0.069)\n",
      "Model Accuracy 4: 0.982 (+/- 0.034)\n",
      "Model Accuracy 5: 0.987 (+/- 0.035)\n",
      "Model Accuracy 6: 1.000 (+/- 0.000)\n",
      "Model Accuracy 7: 0.990 (+/- 0.025)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "model_lst = [LR_classifier,    # 0\n",
    "             KNN_classifier,   # 1\n",
    "             SVML_classifier,  # 2\n",
    "             K_SVM_classifier, # 3\n",
    "             NB_classifier,    # 4\n",
    "             DTC_classifier,   # 5\n",
    "             RF_classifier,    # 6\n",
    "             xgb_classifier]   # 7\n",
    "            #  CB_classifier]    # 8\n",
    "msg = []\n",
    "for i in range(len(model_lst)):\n",
    "    accuracies = cross_val_score(estimator =model_lst[i] , X = X_train, y = y_train, cv = 10)\n",
    "    msg.append(f\"Model Accuracy {i}: %0.3f (+/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))\n",
    "    \n",
    "for i in msg:\n",
    "    print(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy 0: 0.932 (+/- 0.064)\n",
      "Model Accuracy 1: 0.982 (+/- 0.041)\n",
      "Model Accuracy 2: 1.000 (+/- 0.000)\n",
      "Model Accuracy 3: 0.854 (+/- 0.069)\n",
      "Model Accuracy 4: 0.982 (+/- 0.034)\n",
      "Model Accuracy 5: 0.987 (+/- 0.035)\n",
      "Model Accuracy 6: 1.000 (+/- 0.000)\n",
      "Model Accuracy 7: 0.990 (+/- 0.025)\n"
     ]
    }
   ],
   "source": [
    "for i in msg:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the script to compare result with prediction\n",
    "comparision = pd.DataFrame(columns = ['Result', 'Prediction'])\n",
    "comparision.Prediction = pd.Series(y_pred)\n",
    "comparision.Result = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "---\n",
    "Base on the critera we can choose what type of parameter tuning algorithm to use.\n",
    "#### Grid Search: Entropy \n",
    "* Meant to maximize the information content (in random forest we maximize the info. at every split)\n",
    "* pip install joblib\n",
    "* update joblib if there are some problem in GridSearch\n",
    "\n",
    "#### Grid Search: Gini\n",
    "* Meant to minimize the probability of mislabelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input parameters\n",
    "---\n",
    "* Input different parameter based on the \"Model\" that you tring to tune\n",
    "* Look into documentaion of specific algorithm for avaiable parameters\n",
    "* Based on the best found parametes, slim down the range of the setting and test again\n",
    ". \n",
    "         ***1st setting***\n",
    "         {\"max_depth\": [3, None], \n",
    "         \"max_features\": [1, 5, 10], \n",
    "         'min_samples_split': [2, 5, 10],\n",
    "         'min_samples_leaf': [1, 5, 10], \n",
    "         \"bootstrap\": [True, False],  \n",
    "         \"criterion\": [\"entropy\"]}\n",
    ".        \n",
    "          ***Slimed down version***\n",
    "          {\"max_depth\": [None],\n",
    "              \"max_features\": [3, 5, 7],\n",
    "              'min_samples_split': [8, 10, 12],\n",
    "              'min_samples_leaf': [1, 2, 3],\n",
    "              \"bootstrap\": [True],\n",
    "              \"criterion\": [\"entropy\"]}\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 5, 10],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 5, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"entropy\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search: Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search_entropy = GridSearchCV(estimator = RF_classifier, # Make sure classifier points to the RF model\n",
    "                           param_grid = parameters,\n",
    "                           scoring = \"accuracy\",\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "t0 = time.time()    # RECORD THE DURATION ALGORITHM TOOK\n",
    "grid_search = grid_search.fit(X_train, y_train) \n",
    "t1 = time.time()\n",
    "print(\"Took %0.2f seconds\" % (t1 - t0))\n",
    "\n",
    "rf_best_accuracy = grid_search.best_score_\n",
    "rf_best_parameters = grid_search.best_params_\n",
    "rf_best_accuracy, rf_best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search: Gini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 5, 10],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 5, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search_gini = GridSearchCV(estimator = classifier, # Make sure classifier points to the RF model\n",
    "                           param_grid = parameters,\n",
    "                           scoring = \"accuracy\",\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "t0 = time.time()\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print(\"Took %0.2f seconds\" % (t1 - t0))\n",
    "\n",
    "rf_best_accuracy = grid_search.best_score_\n",
    "rf_best_parameters = grid_search.best_params_\n",
    "rf_best_accuracy, rf_best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('body_language.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search_entropy, f)\n",
    "\n",
    "with open('body_language.pkl', 'rb') as f:\n",
    "    grid_search_entropy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Test Set\n",
    "---\n",
    "* Base on the best grid result test model, using its specific grid_serach parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Correct grid_search 'entropy' or 'gini'\n",
    "y_pred = grid_search_entropy.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sn.set(font_scale=2.8)\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "plt.figure(figsize = (50,40))\n",
    "sn.heatmap(corr, annot=True,mask=mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.concat([y_test, users], axis = 1).dropna()\n",
    "final_results['predictions'] = y_pred\n",
    "final_results = final_results[['entry_id', 'e_signed', 'predictions']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
